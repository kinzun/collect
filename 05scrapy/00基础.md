



## 什么是Scrapy？

　　Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。

## 安装

```
pip install scrapy
```

## 创建项目

在开始爬取之前，您必须创建一个新的Scrapy项目。 进入您打算存储代码的目录中，运行下列命令:

```shell
scrapy startproject tutorial
```

该命令将会创建包含下列内容的 `tutorial` 目录:

```
tutorial/
    scrapy.cfg
    tutorial/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ... 
```

这些文件分别是:

- `scrapy.cfg`   项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）
- `items.py`     设置数据存储模板，用于结构化数据，如：Django的Model
- `pipelines `   数据持久化处理
- `settings.py`  配置文件，如：递归的层数、并发数，延迟下载等
- `spiders  `    爬虫目录，如：创建文件，编写爬虫解析规则





Spider 是用户编写用于从单个网站(或者一些网站)爬取数据的类。

```python
You can start your first spider with:
    cd tutorial
    
    scrapy genspider example example.com

```

以下为我们的第一个Spider代码，保存在 `tutorial/spiders` 目录下的 `dmoz_spider.py` 文件中:

```python

# -*- coding: utf-8 -*-
import scrapy

class QiubaiSpider(scrapy.Spider):
    name = 'qupa' #应用名称
    #允许爬取的域名（如果遇到非该域名的url则爬取不到数据）
    allowed_domains = ['https://www.qupa.com/']
    #起始爬取的url
    start_urls = ['https://www.qupa.com/']

     #访问起始URL并获取结果后的回调函数，该函数的response参数就是向起始的url发送请求后，获取的响应对象.该函数返回值必须为可迭代对象或者NUll 
     def parse(self, response):
        print(response.text) #获取字符串类型的响应内容
        print(response.body)#获取字节类型的相应内容
```



为了创建一个 Spider ，您必须继承 [`scrapy.Spider`](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider) 类， 且定义以下三个属性:

- [`name`](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider.name): 用于区别 Spider。 该名字必须是唯一的，您不可以为不同的 Spider设定相同的名字。

- [`start_urls`](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider.start_urls): 包含了 Spider 在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。

- [`parse()`](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider.parse) 是 spider 的一个方法。 被调用时，每个初始URL完成下载后生成的 [`Response`](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/request-response.html#scrapy.http.Response) 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据( response data )，提取数据(生成 item )以及生成需要进一步处理的URL的 [`Request`](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/request-response.html#scrapy.http.Request) 对象。

  [文档](https://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html#spider)





修改配置 `settings.py`

```
修改内容及其结果如下：

19行：
USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
#伪装请求头

22行：
ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议
```



爬取启动

```
    scrapy crawl <qupa> ：该种执行形式会显示执行的日志信息
    scrapy crawl <爬虫名称> --nolog：该种执行形式不会显示执行的日志信息
```







- [命令行工具(Command line tools)](https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/commands.html)